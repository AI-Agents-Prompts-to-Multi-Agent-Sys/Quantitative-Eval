{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Usage\n",
    "1. Config your Google API key in the secrets\n",
    "2. Run the first two cells to initialize the environment\n",
    "3. Run either the third or fourth cell to initialize the personas and the evaluation criteria\n",
    "4. Run the remaining cells to execute the evaluation and visualize the results"
   ],
   "id": "3012def33677d63c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!pip install -q -r https://raw.githubusercontent.com/AI-Agents-Prompts-to-Multi-Agent-Sys/Quantitative-Eval/master/requirements.txt",
   "id": "4bf7d69643165104",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import json\n",
    "import operator\n",
    "import re\n",
    "from typing import TypedDict, Annotated, List\n",
    "\n",
    "import pandas as pd\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langgraph.constants import END, START\n",
    "from langgraph.graph import StateGraph\n",
    "from tqdm import tqdm\n",
    "from tenacity import retry, stop_after_attempt\n",
    "\n",
    "# Load environment variables (GOOGLE_API_KEY should be set either in .env file or in the secrets)\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_API_KEY')\n",
    "except ImportError:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "except KeyError:\n",
    "    raise KeyError(\"Please set the GOOGLE_API_KEY in your secrets.\")\n",
    "\n",
    "# LLM config\n",
    "# At here you can change the model, tweak its parameters, or even use different LLM provider\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-preview-05-20\", temperature=0.7)"
   ],
   "id": "54c3d82acd1ccca7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Use One of the following two cells to initialize the personas and the evaluation criteria.",
   "id": "ed1c6cec5d5282d3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Initialize personas and criteria by a llm\n",
    "# To be implemented"
   ],
   "id": "c923589bfb3eec25",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Initialize personas and criteria manually\n",
    "\n",
    "# What to evaluate\n",
    "EVALUATION_SUBJECT = \"band\"\n",
    "\n",
    "# List of items to evaluate\n",
    "ITEMS = [\n",
    "    \"The Beatles\", \"Led Zeppelin\", \"Pink Floyd\", \"Queen\", \"The Rolling Stones\",\n",
    "    \"Metallica\", \"Megadeth\", \"Black Sabbath\", \"Iron Maiden\", \"Tool\"\n",
    "]\n",
    "\n",
    "# Persona definitions\n",
    "# Each persona has generates a call to the LLM, so please be aware of the rate limits of your LLM provider\n",
    "# For Gemini, the rate limit is 10 calls per minute for 2.5 flash models and 5 calls per minute for 2.5 pro models.\n",
    "PERSONAS = {\n",
    "    \"metalhead\": \"You're in your 30s, a lifelong metal fan. You value power, aggression, instrumental mastery, and complexity. You dismiss pop and overproduced music as shallow.\",\n",
    "    \"popstar\": \"You're in your 20s, immersed in social media culture. You love global accessibility, emotional resonance, and catchy choruses. You believe great bands bring joy and unity.\",\n",
    "    \"boomer\": \"You're in your 70s. You grew up during the golden age of rock and believe greatness is rooted in legacy, songwriting, and timeless appeal. Newer music feels synthetic to you.\",\n",
    "    \"genz\": \"You're a teenager, online-native, and value diversity, identity, and innovation in music. You're drawn to bands that say something real or break genre rules.\",\n",
    "    \"indie\": \"You're in your 30s, an art-school type who craves authenticity, emotion, and underground cool. You dislike commercial polish and love expressive weirdness.\",\n",
    "}\n",
    "\n",
    "# Criteria for evaluation\n",
    "CRITERIA = {\n",
    "    \"Musical Innovation\": \"Pioneering ideas, new sounds, genre blending.\",\n",
    "    \"Cultural Impact\": \"Broader societal influence, pop culture penetration.\",\n",
    "    \"Lyrical or Thematic Depth\": \"Narrative richness, philosophical weight, relatability.\",\n",
    "    \"Technical Proficiency\": \"Musical complexity, virtuosity, performance execution.\",\n",
    "    \"Live Performance Strength\": \"Energy, presence, crowd connection on stage.\",\n",
    "    \"Legacy & Longevity\": \"Enduring influence across generations and artists.\"\n",
    "}\n",
    "\n",
    "# Instructions/background information for the personas\n",
    "PERSONA_ROLE = \"music critic\"\n",
    "INSTRUCTION = \"You have been asked to evaluate the greatness of 21 historically significant bands across genres including rock, metal, pop, and progressive.\"\n",
    "\n",
    "\n",
    "# Prompt template\n",
    "def make_prompt(persona_description):\n",
    "    return f\"\"\"You are a {PERSONA_ROLE} with the following worldview:\n",
    "\n",
    "{persona_description}\n",
    "\n",
    "{INSTRUCTION}\n",
    "\n",
    "Evaluate each {EVALUATION_SUBJECT} based on the following {len(CRITERIA)} criteria, scoring from 1 (low) to 5 (high):\n",
    "\n",
    "{\"\".join(f\"{key}: {value}{chr(10)}\" for key, value in CRITERIA.items())}\n",
    "Here are the {EVALUATION_SUBJECT}s to evaluate:\n",
    "{chr(10).join('- ' + item for item in ITEMS)}\n",
    "\n",
    "Please respond ONLY in the following strict JSON format:\n",
    "\n",
    "```json\n",
    "{{\n",
    "  \"ratings\": [\n",
    "    {{\n",
    "      \"item\": \"the corresponding {EVALUATION_SUBJECT} name here\"{\"\".join(f',{chr(10)}      \"{criteria}\": int' for criteria in CRITERIA)}\n",
    "    }},\n",
    "    // ...More {EVALUATION_SUBJECT} evaluations here\n",
    "  ],\n",
    "  \"justification\": \"Your paragraph explaining the ratings here.\",\n",
    "  \"ranking\": [\"{EVALUATION_SUBJECT}1\", \"{EVALUATION_SUBJECT}2\", ..., \"{EVALUATION_SUBJECT}{len(ITEMS)}\"]\n",
    "}}\n",
    "```\n",
    "\n",
    "- The ratings list must include all {len(ITEMS)} {EVALUATION_SUBJECT}s.\n",
    "- The ranking list must be in your personal order (1st to {len(ITEMS)}st).\n",
    "- Do not include any commentary outside the JSON block.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Example prompt:\")\n",
    "print(make_prompt(list(PERSONAS.values())[0]))\n"
   ],
   "id": "a23f165a748c41f3",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2696d0acd5ea46f3",
   "metadata": {},
   "source": [
    "\n",
    "# Clean LLM output\n",
    "def clean_json_string(text: str) -> str:\n",
    "    cleaned = re.sub(r\"```(?:json)?\", \"\", text)\n",
    "    return cleaned.replace(\"```\", \"\").strip()\n",
    "\n",
    "# Parse JSON\n",
    "def parse_json_response(response):\n",
    "    response_cleaned = clean_json_string(response)\n",
    "    data = json.loads(response_cleaned)\n",
    "    ratings = data[\"ratings\"]\n",
    "    justification = data[\"justification\"]\n",
    "    ranking = data[\"ranking\"]\n",
    "\n",
    "    ranking_column = []\n",
    "    for i, item in enumerate(ranking):\n",
    "        ranking_column += [{\"item\": item, \"rank\": i + 1}]\n",
    "\n",
    "    df = pd.DataFrame(ratings)\n",
    "    df = pd.merge(df, pd.DataFrame(ranking_column), on=\"item\", how=\"left\")\n",
    "    df.columns = [EVALUATION_SUBJECT] + list(CRITERIA.keys()) + [\"Rank\"]\n",
    "    return df, justification\n",
    "\n",
    "# Get llm response\n",
    "@retry(stop=stop_after_attempt(3))\n",
    "async def get_llm_response(prompt):\n",
    "    response = await llm.ainvoke(prompt)\n",
    "    return parse_json_response(response.content)\n",
    "\n",
    "# State definition\n",
    "class Vote(TypedDict):\n",
    "    df: pd.DataFrame\n",
    "    justification: str\n",
    "    persona: str\n",
    "\n",
    "class State(TypedDict):\n",
    "    votes: Annotated[List[Vote], operator.add]\n",
    "\n",
    "# Initialize progress bar\n",
    "try:\n",
    "    pbar.close()\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "pbar = tqdm(\"Evaluating bands with personas\", total=len(PERSONAS), unit=\"persona\")\n",
    "\n",
    "# Agent node\n",
    "def make_agent_node(persona_key):\n",
    "    async def node(state):\n",
    "        persona = PERSONAS[persona_key]\n",
    "        prompt = make_prompt(persona)\n",
    "        df, justification = await get_llm_response(prompt)\n",
    "\n",
    "        state['votes'] = [{\n",
    "                \"df\": df,\n",
    "                \"justification\": justification,\n",
    "                \"persona\": persona_key,\n",
    "        }]\n",
    "\n",
    "        pbar.update(1)\n",
    "        return state\n",
    "    return node\n",
    "\n",
    "# Graph build\n",
    "agent_keys = list(PERSONAS.keys())\n",
    "\n",
    "graph = StateGraph(State)\n",
    "for agent in agent_keys:\n",
    "    graph.add_node(agent, make_agent_node(agent))\n",
    "\n",
    "# Graph edges\n",
    "for agent in agent_keys:\n",
    "    graph.add_edge(START, agent)\n",
    "graph.add_edge([agent for agent in agent_keys], END)\n",
    "\n",
    "# Run\n",
    "compiled = graph.compile()\n",
    "result = await compiled.ainvoke({\n",
    "    \"votes\": [],\n",
    "})\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "dac7354e0e335825",
   "metadata": {},
   "source": [
    "for agent in result:\n",
    "    if agent.endswith(\"_ranking\"):\n",
    "        persona = agent.replace(\"_ranking\", \"\")\n",
    "        ranking = result[agent]\n",
    "        print(f\"üé§ {persona.upper()} Top 5: {ranking[:5]}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "66e5145eefba7ba8",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "result[\"final_scores\"].plot(kind=\"bar\", figsize=(14, 6), legend=False)\n",
    "plt.title(\"Final Rank Scores (Total Points)\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "90636900110fb752",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Assemble the heatmap DataFrame\n",
    "rank_matrix = []\n",
    "for agent in PERSONAS:\n",
    "    ranking = result[f\"{agent}_ranking\"]\n",
    "    rank_row = {band: i + 1 for i, band in enumerate(ranking)}\n",
    "    rank_matrix.append(rank_row)\n",
    "\n",
    "rank_df = pd.DataFrame(rank_matrix, index=PERSONAS.keys())\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.heatmap(rank_df, cmap=\"coolwarm\", annot=True, fmt=\"d\", cbar_kws={\"label\": \"Rank (lower is better)\"})\n",
    "plt.title(\"Band Rankings by Persona\")\n",
    "plt.xlabel(\"Band\")\n",
    "plt.ylabel(\"Persona\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3e8f73779fc72a1f",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "\n",
    "# Compute standard deviation of rankings across agents\n",
    "band_ranks = {band: [] for band in ITEMS}\n",
    "for agent in PERSONAS:\n",
    "    for i, band in enumerate(result[f\"{agent}_ranking\"]):\n",
    "        band_ranks[band].append(i + 1)\n",
    "\n",
    "stats = {\n",
    "    band: {\n",
    "        \"mean_rank\": np.mean(ranks),\n",
    "        \"std_dev\": np.std(ranks),\n",
    "        \"min_rank\": min(ranks),\n",
    "        \"max_rank\": max(ranks)\n",
    "    }\n",
    "    for band, ranks in band_ranks.items()\n",
    "}\n",
    "\n",
    "rank_stats_df = pd.DataFrame(stats).T.sort_values(\"std_dev\", ascending=False)\n",
    "rank_stats_df\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f3db13307f296ccf",
   "metadata": {},
   "source": [
    "df = result[\"summary_df\"]\n",
    "df.head()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for col in df.columns:\n",
    "    print(f\"\\nüîπ Top 5 bands by {col.upper()}:\")\n",
    "    display(df.sort_values(col, ascending=False).head(5))\n"
   ],
   "id": "bbc87dd687916862",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df[\"std_dev\"] = df.std(axis=1)\n",
    "df[\"mean\"] = df.mean(axis=1)\n",
    "df_consistency = df.sort_values(\"std_dev\")\n",
    "df_consistency[[\"mean\", \"std_dev\"]]\n"
   ],
   "id": "d83f78801f71d518",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "top3 = df.mean(axis=1).sort_values(ascending=False).head(3).index\n",
    "categories = list(df.columns[:-2])  # exclude mean/std_dev\n",
    "\n",
    "for band in top3:\n",
    "    values = df.loc[band, categories].values.flatten().tolist()\n",
    "    values += values[:1]  # repeat first value to close the radar loop\n",
    "\n",
    "    angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()\n",
    "    angles += angles[:1]\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    ax = plt.subplot(111, polar=True)\n",
    "    ax.plot(angles, values, linewidth=2, label=band)\n",
    "    ax.fill(angles, values, alpha=0.3)\n",
    "    ax.set_thetagrids(np.degrees(angles[:-1]), categories)\n",
    "    ax.set_title(f\"{band} Score Profile\")\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ],
   "id": "e27c1d6eb30b83dd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(df[categories].corr(), annot=True, cmap=\"coolwarm\")\n",
    "plt.title(\"Correlation Between Evaluation Criteria\")\n",
    "plt.show()\n"
   ],
   "id": "c08002b56dab40e4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df[\"mean\"] = df.mean(axis=1)\n",
    "df_sorted = df.sort_values(\"mean\", ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=df_sorted[\"mean\"], y=df_sorted.index)\n",
    "plt.title(\"üèÜ Average Score per Band (All Criteria)\")\n",
    "plt.xlabel(\"Average Score\")\n",
    "plt.ylabel(\"Band\")\n",
    "plt.xlim(1, 3)\n",
    "plt.show()\n"
   ],
   "id": "3f597c0e66328659",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "top_bands = df_sorted.head(10)\n",
    "top_bands.iloc[:, :6].plot(kind='barh', stacked=True, figsize=(12, 7), colormap='tab20c')\n",
    "plt.title(\"üéº Top 10 Bands by Score Breakdown\")\n",
    "plt.xlabel(\"Total Score\")\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.show()\n"
   ],
   "id": "35618d57b191907b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "4f608f61ac5dc8fa",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
