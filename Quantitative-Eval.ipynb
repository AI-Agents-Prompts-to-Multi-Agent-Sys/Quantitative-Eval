{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# About\n",
    "This notebook implements an AI-agent workflow that enables evaluation of any set of items using a group of customized personas and criteria. In the workflow, each persona is prompted to rate the given items under the defined criteria using a score from 1 to 5. After rating, they also provide a personal ordering of the items.\n",
    "\n",
    "The overall workflow proceeds as follows:\n",
    "\n",
    "1. **Prompt setup**\n",
    "    1. Define the evaluation subject and items\n",
    "    2. Create persona definitions and evaluation criteria\n",
    "    3. Set up a prompt template for the personas\n",
    "2. **Generate evaluations**\n",
    "    1. For each persona, apply the prompt template and make an API call\n",
    "    2. Parse and store the returned evaluations\n",
    "3. **Process evaluations**\n",
    "    1. Normalize the results\n",
    "    2. Apply weights to each persona or criterion (if applicable)\n",
    "4. **Display the results**"
   ],
   "metadata": {
    "id": "iP-QfH2Pd-JI"
   },
   "id": "iP-QfH2Pd-JI"
  },
  {
   "metadata": {
    "id": "c682ee25d4377035"
   },
   "cell_type": "markdown",
   "source": [
    "# Instructions\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Follow the steps below to set up and use this notebook properly. These instructions will help ensure everything works smoothly, especially if you're using Google Colab for the first time.\n",
    "\n",
    "\n",
    "## I. Save a Copy to Your Own Google Drive\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "To make changes to this notebook and save your progress, you need to create your own editable copy.\n",
    "Click `Copy to Drive` on top to do so\n",
    "![copy_to_drive.png](https://raw.githubusercontent.com/AI-Agents-Prompts-to-Multi-Agent-Sys/Quantitative-Eval/master/images/copy_to_drive.png)\n",
    "\n",
    "## II. Get a Google API key\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "> *Skip this step if you already have a valid API key created in Google AI Studio.*\n",
    "\n",
    "To use Gemini in this notebook, you'll need to get an API key from Google AI Studio.\n",
    "\n",
    "1. On the left side panel of this notebook, click the `Secrets` ![secrets.png](https://raw.githubusercontent.com/AI-Agents-Prompts-to-Multi-Agent-Sys/Quantitative-Eval/master/images/secrets.png) tab.\n",
    "2. Find and click on Gemini API keys, then select Manage API keys in Google AI Studio.\n",
    "![manage_key_in_google_ai_studio.png](https://raw.githubusercontent.com/AI-Agents-Prompts-to-Multi-Agent-Sys/Quantitative-Eval/master/images/manage_key_in_google_ai_studio.png)\n",
    "This will open a new browser tab directing you to Google AI Studio.\n",
    "3. In the new tab, login to Google AI Studio and click ![create_api_key.png](https://raw.githubusercontent.com/AI-Agents-Prompts-to-Multi-Agent-Sys/Quantitative-Eval/master/images/create_api_key.png) on the top right to create an API key\n",
    "4. Create a new project or select an existing one, then create an API key. Naming can be arbitrary.\n",
    "![creating_api_key.png](https://raw.githubusercontent.com/AI-Agents-Prompts-to-Multi-Agent-Sys/Quantitative-Eval/master/images/creating_api_key.png)\n",
    "5. Click on the API key you just created and copy it to your clipboard.\n",
    "![copy_api_key.png](https://raw.githubusercontent.com/AI-Agents-Prompts-to-Multi-Agent-Sys/Quantitative-Eval/master/images/copy_api_key.png)\n",
    "\n",
    "## III. Add Google API Key to secrets\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "> *If a Google API key already exists in Secrets, simply enable it and skip this part.*\n",
    "\n",
    "Once you have your API key from Google AI Studio, you'll need to import it into this notebook:\n",
    "\n",
    "1. Navigate to the `Secrets` ![secrets.png]() tab on left\n",
    "2. Click on `Add new secret`\n",
    "3. Name the secret `GOOGLE_API_KEY` and paste your API key into the value field\n",
    "4. Turn on the secret by toggling the switch next to it\n",
    "![adding_secret.png](https://raw.githubusercontent.com/AI-Agents-Prompts-to-Multi-Agent-Sys/Quantitative-Eval/master/images/adding_secret.png)\n",
    "\n",
    "## IV. Run the notebook\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Now you' re ready to run the code and interact with the notebook.\n",
    "- Every code cell in this notebook has a **run button** ![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAB4AAAAdCAIAAAAyxktbAAAB5UlEQVR4AeyTL4tCQRTFl0XMBpN+Ai1Gi8EoNotlg8E/RYwWQYOCxSgW/wTDFotNjAaL0aKfQJNh24IY9ocDw7zrzHvCImzY4TKce+45h3nvzXv/ftl6f3vZ+o8Wr/YPvJCv+xLn8m8DTr1arVqtVqFQ+LgvAC2kf6iaOqN3u121Wh2NRvv9/nq9KjWAFpIRAkW6dns05+p2u+fz2WVjhACZSwBvieY4nItZYCFD7JJZoqfTqanOZDLJZNJkTCzE5khG84w8rKkADwaDRqMRjUbBohBjEaRqZfR2u1UDsedyudlsViwWBU/rssjo4/GI2lqhUKhUKo3H42w2awpcFk80vwXXy7Q94ng83mw2e72e/gBYMD4qPdGP498wnuhIJBIOh/3jTqcTX7Xdbh8OB6XEglFhc/dEM0gkEuzWut1u8/m8VqttNhtT4LLIaG6xadN4vV6Xy+XFYqEZDVwWGZ3P52OxmLYpwHcbDoeXy0W15o4Yi8loLKMZVCoVdl1cW/1aNamBEGseYIlOp9P1ep1ZYCFD7JJZopHyjJ1Oh4cFW4sRAmTWqSLt0cw4zmQy4VypVIrrBUMBaCEZIYDxKWe08nCufr+/XC4/7wtAC6mm/ntAtDbzU1C6fQY8G/1MltC8MPoHAAD//3a67YoAAAAGSURBVAMAtzELIMv0a+kAAAAASUVORK5CYII=) that appears in the top-left corner when you hover your mouse over the cell. Click on this button to run the cell.\n",
    "- Alternatively, you can select the cell and press `Shift` + `Enter` to run it.\n",
    "- The outputs of the cells will be displayed below it.\n",
    "- You can use the `Table of contents` tab ![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAB0AAAAbCAIAAAAPqBNFAAABAUlEQVR4AeySQQqCUBiEo70g5GHyAFHrqF21kI4VLqpd0broAHYYA8ED1Mdz9U/xwCfufAwywz+O8r8Zf/o541E/Z8ht9jrs4f8eilexXK8ApHGEPc1+q6o65HntDgRZvsvNbjtbzD3AgE0+b3Jl1kWa3DiO91kWuQNBJpPkfDw97w8PMGCTnzC5zNJpertcAQQZDM0NDpIXNZcaUAYAEWsraXIpADVwdaghSC6a6/aUgREGbPJVkyuzLtLkUgBq4OoQQZBcNNftKQMjDNjkJ0wuM2pAGQAEGQzNDQ6SFzWXGlAGABFrK2lyKQA1GPrABqkBZQAQZDDMfoNTfl/sK/cLAAD//yOd0ZMAAAAGSURBVAMAXYku71pTatsAAAAASUVORK5CYII=) on the left sidebar to jump between sections easily.\n",
    "- Variables that can be modified are marked with `# CHANGE ME`. Read the code comments to understand how and why you might want to adjust that variable.\n",
    "- Keep in mind that **cells are interdependent**. In most cases:\n",
    "  - You must run all previous cells at least once before running the next one.\n",
    "  - Skipping earlier cells may cause later cells to fail or behave unexpectedly.\n",
    "- If you change the contents of a cell, **you need to re-run that cell** for the update to take effect.\n",
    "- **After updating a variable in one cell, you must re-run any other cells that depend on that variable** for the changes to be reflected in their output."
   ],
   "id": "c682ee25d4377035"
  },
  {
   "metadata": {
    "id": "8c59af8a5b6e8d6b"
   },
   "cell_type": "markdown",
   "source": [
    "# Environment setup\n",
    "This part sets up the running environment of this notebook, which includes\n",
    "- Installing and importing the dependencies\n",
    "- Defines utils used in this notebook\n",
    "- Setting up the LLM"
   ],
   "id": "8c59af8a5b6e8d6b"
  },
  {
   "metadata": {
    "id": "c571155ae711af16"
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Install dependencies\n",
    "!pip install -q -r https://raw.githubusercontent.com/AI-Agents-Prompts-to-Multi-Agent-Sys/Quantitative-Eval/master/requirements.txt"
   ],
   "id": "c571155ae711af16"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T01:17:02.790398Z",
     "start_time": "2025-07-03T01:16:59.798202Z"
    },
    "id": "54c3d82acd1ccca7"
   },
   "cell_type": "code",
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import json\n",
    "import operator\n",
    "import re\n",
    "from copy import deepcopy\n",
    "from typing import TypedDict, Annotated, List\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langgraph.constants import END, START\n",
    "from langgraph.graph import StateGraph\n",
    "from tqdm import tqdm\n",
    "from tenacity import retry, stop_after_attempt\n",
    "\n",
    "def clean_json_string(text: str) -> str:\n",
    "  \"\"\"\n",
    "  Cleans the markdowns around a json response\n",
    "  \"\"\"\n",
    "  cleaned = re.sub(r\"```(?:json)?\", \"\", text)\n",
    "  return cleaned.replace(\"```\", \"\").strip()\n",
    "\n",
    "# Load environment variables (GOOGLE_API_KEY should be set either in .env file or in the secrets)\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_API_KEY')\n",
    "except ImportError:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "\n",
    "# LLM config\n",
    "# At here you can change the model, tweak its parameters, or even use different LLM provider\n",
    "# Options for Gemini models include:\n",
    "# - \"gemini-2.5-flash\"\n",
    "# - \"gemini-2.5-pro\"\n",
    "# replace the string after model= with the model you want to use\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0.7)"
   ],
   "id": "54c3d82acd1ccca7",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "If this cell above throws out an error, please check if\n",
    "- `GOOGLE_API_KEY` is set and **it's name matches exactly**\n",
    "- Access to `GOOGLE_API_KEY` is granted\n",
    "\n",
    "You could also read the error message for more details"
   ],
   "metadata": {
    "id": "U9YBTC1H2hn-"
   },
   "id": "U9YBTC1H2hn-"
  },
  {
   "metadata": {
    "id": "ed1c6cec5d5282d3"
   },
   "cell_type": "markdown",
   "source": [
    "# Prompt setup\n",
    "This part sets up the prompt template for the personas, which includes the following steps:\n",
    "- Set up the evaluation subject and items\n",
    "- Set up the personas and criteria\n",
    "- Establish the prompt template"
   ],
   "id": "ed1c6cec5d5282d3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, set up the evaluation subject"
   ],
   "metadata": {
    "id": "Ham7CXVwT75z"
   },
   "id": "Ham7CXVwT75z"
  },
  {
   "cell_type": "code",
   "source": [
    "# What to evaluate\n",
    "EVALUATION_SUBJECT = \"evaluation subject here\" # CHANGE ME"
   ],
   "metadata": {
    "id": "3tek5yxPUBMv"
   },
   "id": "3tek5yxPUBMv",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "id": "a7abbc9756ca4ba2"
   },
   "cell_type": "markdown",
   "source": [
    "Then use **one** of the following cells to set up items to evaluate\n",
    "1. Set up the evaluation items manually\n",
    "2. Set them up with an LLM"
   ],
   "id": "a7abbc9756ca4ba2"
  },
  {
   "metadata": {
    "id": "e60a304581875d42"
   },
   "cell_type": "code",
   "source": [
    "# CHANGE ME\n",
    "# List of items to evaluate\n",
    "ITEMS = [\n",
    "    \"Item 1\",\n",
    "    \"Item 2\",\n",
    "    \"Item 3\",\n",
    "    \"Item 4\",\n",
    "    \"Item 5\",\n",
    "]"
   ],
   "id": "e60a304581875d42",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Number of items to generate\n",
    "num_items = 10 # CHANGE ME\n",
    "\n",
    "# CHANGE ME\n",
    "# This is the prompt sent to the LLM to generate a list of items.\n",
    "# Modifying this prompt may lead to different preferences when selecting items.\n",
    "# IMPORTANT: Do not change part for the response format, as it may break the downstream code.\n",
    "prompt = f\"\"\"\n",
    "Please give the top {num_items} {EVALUATION_SUBJECT}s in public perception\n",
    "\n",
    "Your response should be in two parts:\n",
    "1. Your response: give your choices along with explanations\n",
    "2. Item list: your choices in a list\n",
    "\n",
    "Your response should follow the following JSON format\n",
    "\n",
    "```json\n",
    "{{\n",
    "  \"response\": \"your choices along with explanations\",\n",
    "  \"items\": [\"item1\", \"item2\", ..., \"item{num_items}\"]\n",
    "}}\n",
    "}}\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "@retry(stop=stop_after_attempt(3))\n",
    "def generate_items():\n",
    "    response = llm.invoke(prompt)\n",
    "    response_cleaned = clean_json_string(response.content)\n",
    "    data = json.loads(response_cleaned)\n",
    "    return data[\"response\"], data[\"items\"]\n",
    "\n",
    "items_explained, ITEMS = generate_items()\n",
    "print(items_explained)"
   ],
   "metadata": {
    "id": "1vyL_3uLSjG3"
   },
   "id": "1vyL_3uLSjG3",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "id": "bdded21db99935c5"
   },
   "cell_type": "markdown",
   "source": [
    "Next, use **one** of the following two cells to initialize the personas and the evaluation criteria.\n",
    "1. Set up the personas and criteria manually\n",
    "2. Set them up with an LLM"
   ],
   "id": "bdded21db99935c5"
  },
  {
   "cell_type": "code",
   "source": [
    "# Initialize personas and criteria manually\n",
    "\n",
    "# CHANGE ME\n",
    "# Persona definitions\n",
    "# Each persona has generates a call to the LLM, so please be aware of the rate limits of your LLM provider\n",
    "# For Gemini, the rate limit is 10 calls per minute for 2.5 flash models and 5 calls per minute for 2.5 pro models.\n",
    "PERSONAS = {\n",
    "    \"persona_1\": \"description of persona 1 here\",\n",
    "    \"persona_2\": \"description of persona 2 here\",\n",
    "    \"persona_3\": \"description of persona 3 here\",\n",
    "    \"persona_4\": \"description of persona 4 here\",\n",
    "    \"persona_5\": \"description of persona 5 here\",\n",
    "}\n",
    "\n",
    "# CHANGE ME\n",
    "# Criteria for evaluation\n",
    "CRITERIA = {\n",
    "    \"criterion_1\": \"definition of criterion 1 here\",\n",
    "    \"criterion_2\": \"definition of criterion 2 here\",\n",
    "    \"criterion_3\": \"definition of criterion 3 here\",\n",
    "    \"criterion_4\": \"definition of criterion 4 here\",\n",
    "    \"criterion_5\": \"definition of criterion 5 here\",\n",
    "}\n",
    "\n",
    "# CHANGE ME\n",
    "# Instructions/background information for the personas\n",
    "PERSONA_ROLE = \"Describe the personas' identity in short words, eg. critics\"  # CHANGE ME\n",
    "# CHANGE ME\n",
    "INSTRUCTION = f\"here you give a base instruction for the persona s, eg. you were asked to evaluate {len(ITEMS)} items\"  # CHANGE ME"
   ],
   "metadata": {
    "id": "FhdzDdEuFYQR"
   },
   "id": "FhdzDdEuFYQR",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "id": "c923589bfb3eec25"
   },
   "cell_type": "code",
   "source": [
    "# Initialize personas and criteria by a llm\n",
    "\n",
    "# Each persona has generates a call to the LLM, so please be aware of the rate limits of your LLM provider\n",
    "# For Gemini, the rate limit is 10 calls per minute for 2.5 flash models and 5 calls per minute for 2.5 pro models.\n",
    "# For Gemini rate limit details, visit https://ai.google.dev/gemini-api/docs/rate-limits\n",
    "persona_nums = 5   # CHANGE ME\n",
    "criterion_nums = 5  # CHANGE ME\n",
    "\n",
    "prompt = f\"\"\"\n",
    "You are setting up an experiment that asks diverse group of virtual individuals to evaluate {len(ITEMS)} {EVALUATION_SUBJECT}s. You now need to set define these personas and the criteria for them to follow\n",
    "\n",
    "The definition of personas should follow the following principle:\n",
    "- The group should have a great diversity so that they could reflect the diverse opinions on {EVALUATION_SUBJECT}s.\n",
    "- Do not give simular/repeated persona definitions.\n",
    "- You are speaking to the personas when you give their definitions. Your persona definitions should start with \"You're\".\n",
    "\n",
    "The definition of the criteria should follow the following principle:\n",
    "- Be clear and specific\n",
    "- Each criterion should measure a unique aspect\n",
    "- The personas will fill in a number from 1-5 for each criterion, so make sure each it can be answered with a number\n",
    "- The higher the score, the more that {EVALUATION_SUBJECT} is preferred.\n",
    "\n",
    "You are asked to give {persona_nums} persona definitions and {criterion_nums} criteria\n",
    "\n",
    "Your response should follow the following JSON format\n",
    "```json\n",
    "{{\n",
    "    \"personas\": {{\n",
    "        \"persona_name_here\": \"persona's word view here\",\n",
    "        // ...More personas\n",
    "    }},\n",
    "    \"criteria\": {{\n",
    "        \"criterion_name_here\": \"criterion definition here\",\n",
    "        // ... More criteria\n",
    "    }},\n",
    "    \"persona_role\": \"Describe the personas' identity in short words, eg. critics. You are not supposed to say they are virtual\",\n",
    "    \"instruction\": \"here you give a base instruction for the persona s, eg. you were asked to evaluate {EVALUATION_SUBJECT}s\"\n",
    "}}\n",
    "```\n",
    "\n",
    "Do not include any commentary outside the JSON block.\n",
    "\"\"\"\n",
    "\n",
    "@retry(stop=stop_after_attempt(3))\n",
    "def generate_persona_criteria():\n",
    "    response = llm.invoke(prompt)\n",
    "    response_cleaned = clean_json_string(response.content)\n",
    "    data = json.loads(response_cleaned)\n",
    "    return data[\"personas\"], data['criteria'], data['persona_role'], data['instruction']\n",
    "\n",
    "PERSONAS, CRITERIA, PERSONA_ROLE, INSTRUCTION = generate_persona_criteria()\n",
    "\n",
    "print(\"================== Personas ==================\")\n",
    "for persona, description in PERSONAS.items():\n",
    "    print(f\"{persona}: {description}\")\n",
    "print(\"================== Criteria ==================\")\n",
    "for criterion, description in CRITERIA.items():\n",
    "    print(f\"{criterion}: {description}\")\n",
    "print(\"================== Persona role ==================\")\n",
    "print(PERSONA_ROLE)\n",
    "print(\"================== Instruction ==================\")\n",
    "print(INSTRUCTION)"
   ],
   "id": "c923589bfb3eec25",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "3db76147acfc063a"
   },
   "cell_type": "markdown",
   "source": [
    "Establish the template and print the test prompt.\n",
    "\n",
    "This cell creates the make prompt function, which takes a persona description and applies the template and the template variables we set before. This cell will output the template for the first persona as an example.\n"
   ],
   "id": "3db76147acfc063a"
  },
  {
   "metadata": {
    "id": "6146343afd80a017"
   },
   "cell_type": "code",
   "source": [
    "# CHANGE ME...?\n",
    "# Yes, you could change the prompt template for the personas, but it probably won't make much difference and might break the code\n",
    "# If really want to change this, make sure not to modifiy the format of the outputed JSON.\n",
    "def make_prompt(persona_description):\n",
    "    return f\"\"\"You are a {PERSONA_ROLE} with the following worldview:\n",
    "\n",
    "{persona_description}\n",
    "\n",
    "{INSTRUCTION}\n",
    "\n",
    "Evaluate each {EVALUATION_SUBJECT} based on the following {len(CRITERIA)} criteria, scoring from 1 (low) to 5 (high):\n",
    "\n",
    "{\"\".join(f\"{key}: {value}{chr(10)}\" for key, value in CRITERIA.items())}\n",
    "Here are the {EVALUATION_SUBJECT}s to evaluate:\n",
    "{chr(10).join('- ' + item for item in ITEMS)}\n",
    "\n",
    "Please respond ONLY in the following strict JSON format:\n",
    "\n",
    "```json\n",
    "{{\n",
    "  \"ratings\": [\n",
    "    {{\n",
    "      \"item\": \"the corresponding {EVALUATION_SUBJECT} name here, following the ordering in the given list\"{\"\".join(f',{chr(10)}      \"{criteria}\": int' for criteria in CRITERIA)}\n",
    "    }},\n",
    "    // ...More {EVALUATION_SUBJECT} evaluations here\n",
    "  ],\n",
    "  \"justification\": \"Your paragraph explaining the ratings here.\",\n",
    "  \"ranking\": [\"{EVALUATION_SUBJECT}1\", \"{EVALUATION_SUBJECT}2\", ..., \"{EVALUATION_SUBJECT}{len(ITEMS)}\"]\n",
    "}}\n",
    "```\n",
    "\n",
    "- The ratings list must include all {len(ITEMS)} {EVALUATION_SUBJECT}s.\n",
    "- The ranking list must be in your personal order (1st to {len(ITEMS)}th).\n",
    "- Do not include any commentary outside the JSON block.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Example prompt:\")\n",
    "print(make_prompt(list(PERSONAS.values())[0]))"
   ],
   "id": "6146343afd80a017",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "12b4a0d5df2e8209"
   },
   "cell_type": "markdown",
   "source": [
    "# Generate Evaluations\n",
    "This part performs the API calls, parses the responses, and stores them. This process takes around 40s\n",
    "\n",
    "The output displays the scores of items under each criterion by persona(higher is better). The last column named `rank` is the ordering of items given by the persona(1 is the best)."
   ],
   "id": "12b4a0d5df2e8209"
  },
  {
   "cell_type": "code",
   "id": "2696d0acd5ea46f3",
   "metadata": {
    "id": "2696d0acd5ea46f3"
   },
   "source": [
    "print(\"Starting evaluation...\")\n",
    "\n",
    "# Parse JSON\n",
    "def parse_json_response(response):\n",
    "    response_cleaned = clean_json_string(response)\n",
    "    data = json.loads(response_cleaned)\n",
    "    ratings = data[\"ratings\"]\n",
    "    justification = data[\"justification\"]\n",
    "    ranking = data[\"ranking\"]\n",
    "\n",
    "    ranking_column = []\n",
    "    for i, item in enumerate(ranking):\n",
    "        ranking_column += [{\"item\": item, \"rank\": i + 1}]\n",
    "\n",
    "    df = pd.DataFrame(ratings)\n",
    "\n",
    "    df = pd.merge(df, pd.DataFrame(ranking_column), on=\"item\", how=\"left\")\n",
    "    df.columns = [EVALUATION_SUBJECT[0].upper() + EVALUATION_SUBJECT[1:]] + list(CRITERIA.keys()) + [\"Rank\"]\n",
    "    return df, justification\n",
    "\n",
    "# Get llm response\n",
    "@retry(stop=stop_after_attempt(3))\n",
    "async def get_llm_response(prompt):\n",
    "    response = await llm.ainvoke(prompt)\n",
    "    return parse_json_response(response.content)\n",
    "\n",
    "# State definition\n",
    "class Vote(TypedDict):\n",
    "    df: pd.DataFrame\n",
    "    justification: str\n",
    "    persona: str\n",
    "\n",
    "class State(TypedDict):\n",
    "    votes: Annotated[List[Vote], operator.add]\n",
    "\n",
    "# Initialize progress bar\n",
    "try:\n",
    "    pbar.close()\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "pbar = tqdm(f\"Evaluating {EVALUATION_SUBJECT}s with personas\", total=len(PERSONAS), unit=\"persona\")\n",
    "\n",
    "# Agent node\n",
    "def make_agent_node(persona_key):\n",
    "    async def node(state):\n",
    "        persona = PERSONAS[persona_key]\n",
    "        prompt = make_prompt(persona)\n",
    "        df, justification = await get_llm_response(prompt)\n",
    "\n",
    "        state['votes'] = [{\n",
    "                \"df\": df,\n",
    "                \"justification\": justification,\n",
    "                \"persona\": persona_key,\n",
    "        }]\n",
    "\n",
    "        pbar.update(1)\n",
    "        return state\n",
    "    return node\n",
    "\n",
    "# Graph build\n",
    "agent_keys = list(PERSONAS.keys())\n",
    "\n",
    "graph = StateGraph(State)\n",
    "for agent in agent_keys:\n",
    "    graph.add_node(agent, make_agent_node(agent))\n",
    "\n",
    "# Graph edges\n",
    "for agent in agent_keys:\n",
    "    graph.add_edge(START, agent)\n",
    "graph.add_edge([agent for agent in agent_keys], END)\n",
    "\n",
    "# Run\n",
    "compiled = graph.compile()\n",
    "results = await compiled.ainvoke({\n",
    "    \"votes\": [],\n",
    "})\n",
    "\n",
    "votes = results['votes']\n",
    "criteria_keys = list(CRITERIA.keys())\n",
    "\n",
    "# Display results\n",
    "for vote in votes:\n",
    "    print(f\"\\n ================== Evaluation by persona: {vote['persona']} ==================\")\n",
    "    display(vote['df'])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "6d2c349e713329ab"
   },
   "cell_type": "markdown",
   "source": [
    "# Process Evaluations"
   ],
   "id": "6d2c349e713329ab"
  },
  {
   "metadata": {
    "id": "bcb25eb75940937f"
   },
   "cell_type": "markdown",
   "source": [
    "## Optional: Normalize the scores\n",
    "This step reduces the bias from individual scoring tendencies by calculating the z-score of the scores by persona. It then shifts the score so that we don't have negative scores"
   ],
   "id": "bcb25eb75940937f"
  },
  {
   "cell_type": "code",
   "id": "dac7354e0e335825",
   "metadata": {
    "id": "dac7354e0e335825"
   },
   "source": [
    "# Normalize the scores\n",
    "for vote in votes:\n",
    "    all_values = vote['df'][criteria_keys].values.flatten()\n",
    "    mean = all_values.mean()\n",
    "    std_dev = all_values.std()\n",
    "    vote['df'][criteria_keys] = (vote['df'][criteria_keys] - mean) / std_dev\n",
    "\n",
    "# Ensure all scores are non-negative\n",
    "min_z = min([vote['df'][criteria_keys].min().min() for vote in votes])\n",
    "for vote in votes:\n",
    "    vote['df'][criteria_keys] = vote['df'][criteria_keys] - min_z\n",
    "\n",
    "# Display normalized scores\n",
    "for vote in votes:\n",
    "    print(f\"================== Normalized scores for persona: {vote['persona']} ==================\")\n",
    "    display(vote['df'])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "a91fa6d97f38eef9"
   },
   "cell_type": "markdown",
   "source": [
    "## Set the weight for each criterion\n",
    "\n",
    "Here you can change the weight for each criterion to reflect its importance in the overall evaluation.\n",
    "\n",
    "A higher weight means the criterion will have a greater impact on the final score, giving more influence to items that perform well in that area.\n",
    "\n",
    "The scores after this cell would refer to the weighted score.\n",
    "\n",
    "> Please note that the size and order of the weights array must match the number of criteria defined in the `CRITERIA` dictionary."
   ],
   "id": "a91fa6d97f38eef9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following cell prints out your criteria for reference"
   ],
   "metadata": {
    "id": "7mJ_x2SweQmG"
   },
   "id": "7mJ_x2SweQmG"
  },
  {
   "metadata": {
    "id": "6486d5901d5ebd13"
   },
   "cell_type": "code",
   "source": "pd.DataFrame(criteria_keys).T",
   "id": "6486d5901d5ebd13",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Change the `weights` variable to change the weights"
   ],
   "metadata": {
    "id": "PAwiidrggQ0W"
   },
   "id": "PAwiidrggQ0W"
  },
  {
   "metadata": {
    "id": "578bb9da68f76611"
   },
   "cell_type": "code",
   "source": [
    "# weights for each criterion\n",
    "# The length and order of this list MUST MATCH the criteria\n",
    "weights = [1, 1, 1, 1, 1, 1] # CHANGE ME\n",
    "\n",
    "weighted_votes = [ deepcopy(vote) for vote in votes ]\n",
    "\n",
    "for weighted_vote, vote in zip(weighted_votes, votes):\n",
    "    for i, key in enumerate(criteria_keys):\n",
    "        weighted_vote['df'][key] = vote['df'][key] * weights[i]\n",
    "    weighted_vote['df']['score_sum'] = weighted_vote['df'][criteria_keys].sum(axis=1)\n",
    "\n",
    "# Display weighted scores\n",
    "for weighted_vote in weighted_votes:\n",
    "    print(f\"================== Weighted scores for persona: {weighted_vote['persona']} ==================\")\n",
    "    display(weighted_vote['df'])"
   ],
   "id": "578bb9da68f76611",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "44b5ccd26d0ea43c"
   },
   "cell_type": "markdown",
   "source": [
    "## Calculate the final scores\n",
    "Sum up the scores for each item across all personas, and then display the final scores.\n",
    "\n",
    "This table displays the score summary. You could click the ![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACQAAAAnCAIAAADo9n1hAAACkElEQVR4AeyUWXLTQBiEJcd2EhMclsJcghfuAGeBG3AGbgBngTvwwiUwBUmcxEm8ZPKp2p6RZkabo/JTXF3jf+m/W5qR1DN7/PWSPf6ezIxJbpfJ7Mb8vzJ/Z+bPRQYCUoq0INQeSP02rtaZx3RmLubmZpEs18m92cgSkFKkBQFXyJte7K/G7PLW/LvKPGKzfg1XyIz4jW1easY1skXzuy2x8T8jDDIeTsTNlqvk7NqwReFAkwqD2fjK50bMuKjzueE8fG6bnHFEkMoPRcw4Z6givf/yewdoFhGkFGv1zTheNkE9rZ8/TlpBU1qRQlAxa8GMu+Z4qXr49GHSEN4gKYLIEoCC2XyxfYPodAcr68z4BPCidGfhlJBFnNyZ3QVPKm3h+89pQ4gfrhJ3ZotVfA+//Zi2QuhEReLOzB4jvTx4FH99fScQ01KslZSiYlZiKiEk7szW9yGns4rEnRnvYGfagZDEnVlA6L7gzHppXJ2nw36xiCHZlICUIoFATCWExJ3ZgQsLZM48D3oVKS0IISTuHPoHIWdTsd8q5TYl8CpKw1XizmzYL9nHcLR9ReLO7LDfXqPxhMSdWZomx8P4tP1WqW1TAq+i1FuRRZyiMyMZDSM7yQOWB7SKlBYED1a2YMYxjg4LTL5AOyAvgSCyqhTMKD0/SgfljyWEVkAKQTvim9EYH6d6B4kfA0SQyitEzLjrF6PH+uGECFI1ZrQH/eTls933k93LxoN3KXJnmAEu6tVJyvEStwIjDDIeTpWaicrxvj5JeVGUVq/QIDNSRqsxY4xr5Jwn4/R0lLmyRZwHdUBAigctCNAgUy9DvZkm+QQcDRLk2KI34/TtaQYCUoq0IIhZsfYqeg1a7Sh7NXsAAAD//xeG0VMAAAAGSURBVAMACpwqATW6NiAAAAAASUVORK5CYII=) on the top right of the table to convert it to an interactive table and then click on the columns to sort by a different criterion."
   ],
   "id": "44b5ccd26d0ea43c"
  },
  {
   "metadata": {
    "id": "1198f41ed83fce2"
   },
   "cell_type": "code",
   "source": [
    "final_scores = weighted_votes[0]['df'].copy().drop(columns=['score_sum'])\n",
    "for vote in weighted_votes[1:]:\n",
    "    final_scores[criteria_keys] += vote['df'][criteria_keys]\n",
    "\n",
    "final_scores = final_scores.drop(columns=['Rank'])\n",
    "final_scores['total score'] = final_scores[criteria_keys].sum(axis=1)\n",
    "# Sort by total score\n",
    "final_scores = final_scores.sort_values(by='total score', ascending=False)\n",
    "\n",
    "final_scores = final_scores.set_index(EVALUATION_SUBJECT[0].upper() + EVALUATION_SUBJECT[1:])\n",
    "\n",
    "final_scores"
   ],
   "id": "1198f41ed83fce2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "a7bdb1319cf9b5ba"
   },
   "cell_type": "markdown",
   "source": [
    "# Visualization"
   ],
   "id": "a7bdb1319cf9b5ba"
  },
  {
   "metadata": {
    "id": "a76260b57e50f2d0"
   },
   "cell_type": "markdown",
   "source": [
    "## Score Breakdown\n",
    "This chart shows each item's total score and how that score is split among the criteria. Each bar is an item, and its length is the total score. The colored parts show the score for each criterion. This helps you see the total score and which criteria contributed the most. Items are ordered by total score from highest to lowest."
   ],
   "id": "a76260b57e50f2d0"
  },
  {
   "metadata": {
    "id": "3bbf48c8abd4e957"
   },
   "cell_type": "code",
   "source": [
    "final_scores['total score'] = final_scores[criteria_keys].sum(axis=1)\n",
    "final_scores = final_scores.sort_values(by='total score', ascending=True)\n",
    "final_scores = final_scores.drop(columns=['total score'])\n",
    "final_scores.plot(kind='barh', stacked=True, figsize=(12, 7), colormap='tab20c')\n",
    "plt.title(\"Score Breakdown\")\n",
    "plt.xlabel(\"Total Score\")\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.show()"
   ],
   "id": "3bbf48c8abd4e957",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "ce9af0c9e236ac55"
   },
   "cell_type": "markdown",
   "source": [
    "## Radar Chart"
   ],
   "id": "ce9af0c9e236ac55"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The radar charts show how each item scores on different criteria. Each axis stands for a criterion, and the point along that axis shows the score—the farther from the center, the higher the score.\n",
    "\n",
    "For example, a wide, balanced shape that stretches far in all directions means the item scores well across the board. A narrow or uneven shape might show strong performance in some areas but weaknesses in others."
   ],
   "metadata": {
    "id": "6ykpYjJQ-lwr"
   },
   "id": "6ykpYjJQ-lwr"
  },
  {
   "metadata": {
    "id": "35c5e5175409787f"
   },
   "cell_type": "code",
   "source": [
    "items = final_scores.mean(axis=1).sort_values(ascending=False).index\n",
    "\n",
    "for item in items:\n",
    "    values = final_scores.loc[item, criteria_keys].values.flatten().tolist()\n",
    "    values += values[:1]\n",
    "\n",
    "    angles = np.linspace(0, 2 * np.pi, len(criteria_keys), endpoint=False).tolist()\n",
    "    angles += angles[:1]\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    ax = plt.subplot(111, polar=True)\n",
    "    ax.plot(angles, values, linewidth=2, label=item)\n",
    "    ax.fill(angles, values, alpha=0.3)\n",
    "    ax.set_thetagrids(np.degrees(angles[:-1]), criteria_keys)\n",
    "    ax.set_title(f\"{item} Score Profile\")\n",
    "    plt.show()"
   ],
   "id": "35c5e5175409787f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "c9b25b82c4acb69c"
   },
   "cell_type": "markdown",
   "source": [
    "## Heatmap of Criteria Correlation"
   ],
   "id": "c9b25b82c4acb69c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "This heatmap shows how the different evaluation criteria are related to each other. The colors indicate the strength of the relationship:\n",
    "\n",
    "- Red/warm colors mean that if an item scores high on one criterion, they are likely to score high on the other related criterion as well.\n",
    "- Blue/cool colors mean that if an item scores high on one criterion, they are likely to score low on the other, or vice versa.\n",
    "- Colors near the middle mean there's not much of a relationship between the two criteria.\n",
    "\n",
    "This helps you understand if the criteria are measuring similar things or if they are independent aspects of evaluation."
   ],
   "metadata": {
    "id": "-l2qSlp1-8mr"
   },
   "id": "-l2qSlp1-8mr"
  },
  {
   "metadata": {
    "id": "292984ab92d9678d"
   },
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(final_scores[criteria_keys].corr(), annot=True, cmap=\"coolwarm\")\n",
    "plt.title(\"Correlation Between Evaluation Criteria\")\n",
    "plt.show()"
   ],
   "id": "292984ab92d9678d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "6041037d5148b6cf"
   },
   "cell_type": "markdown",
   "source": [
    "## Heatmap of individual rankings\n",
    "\n",
    "This heatmap shows the ranking given by each persona to each item.\n",
    "\n",
    "> Note that rankings are independent of the scores, and a lower number is better (1 means first place).\n",
    "\n",
    "The columns are sorted so that the item with the best average ranking (lowest number) are on the left. Each row represents a persona, and the color and number in each cell show you where that persona ranked each item. It visually highlights which items were consistently ranked high or low across the different personas, and where there were disagreements in ranking."
   ],
   "id": "6041037d5148b6cf"
  },
  {
   "cell_type": "code",
   "id": "90636900110fb752",
   "metadata": {
    "id": "90636900110fb752"
   },
   "source": [
    "rank_matrix = []\n",
    "items = weighted_votes[0]['df'][EVALUATION_SUBJECT[0].upper() + EVALUATION_SUBJECT[1:]].tolist()\n",
    "for weighted_vote in weighted_votes:\n",
    "    rank_row = dict(zip(weighted_vote['df'][EVALUATION_SUBJECT[0].upper() + EVALUATION_SUBJECT[1:]], list(weighted_vote['df']['Rank'])))\n",
    "    rank_matrix.append(rank_row)\n",
    "\n",
    "rank_df = pd.DataFrame(rank_matrix, index=[i['persona'] for i in weighted_votes], columns=items)\n",
    "rank_df = rank_df.loc[:, rank_df.mean().sort_values().index]\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.heatmap(rank_df, cmap=\"coolwarm\", annot=True, fmt=\"d\", cbar_kws={\"label\": \"Rank (lower is better)\"})\n",
    "plt.title(f\"{EVALUATION_SUBJECT[0].upper() + EVALUATION_SUBJECT[1:]} Rankings by Persona\")\n",
    "plt.xlabel(EVALUATION_SUBJECT[0].upper() + EVALUATION_SUBJECT[1:])\n",
    "plt.ylabel(\"Persona\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "f730b3a7fafa00f8"
   },
   "cell_type": "markdown",
   "source": [
    "# Statistics"
   ],
   "id": "f730b3a7fafa00f8"
  },
  {
   "metadata": {
    "id": "c7ac6fb6d00836fc"
   },
   "cell_type": "markdown",
   "source": [
    "## Summary on rankings\n",
    "\n",
    "This table displays the statistics on ranking. You could click the ![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACQAAAAnCAIAAADo9n1hAAACkElEQVR4AeyUWXLTQBiEJcd2EhMclsJcghfuAGeBG3AGbgBngTvwwiUwBUmcxEm8ZPKp2p6RZkabo/JTXF3jf+m/W5qR1DN7/PWSPf6ezIxJbpfJ7Mb8vzJ/Z+bPRQYCUoq0INQeSP02rtaZx3RmLubmZpEs18m92cgSkFKkBQFXyJte7K/G7PLW/LvKPGKzfg1XyIz4jW1easY1skXzuy2x8T8jDDIeTsTNlqvk7NqwReFAkwqD2fjK50bMuKjzueE8fG6bnHFEkMoPRcw4Z6givf/yewdoFhGkFGv1zTheNkE9rZ8/TlpBU1qRQlAxa8GMu+Z4qXr49GHSEN4gKYLIEoCC2XyxfYPodAcr68z4BPCidGfhlJBFnNyZ3QVPKm3h+89pQ4gfrhJ3ZotVfA+//Zi2QuhEReLOzB4jvTx4FH99fScQ01KslZSiYlZiKiEk7szW9yGns4rEnRnvYGfagZDEnVlA6L7gzHppXJ2nw36xiCHZlICUIoFATCWExJ3ZgQsLZM48D3oVKS0IISTuHPoHIWdTsd8q5TYl8CpKw1XizmzYL9nHcLR9ReLO7LDfXqPxhMSdWZomx8P4tP1WqW1TAq+i1FuRRZyiMyMZDSM7yQOWB7SKlBYED1a2YMYxjg4LTL5AOyAvgSCyqhTMKD0/SgfljyWEVkAKQTvim9EYH6d6B4kfA0SQyitEzLjrF6PH+uGECFI1ZrQH/eTls933k93LxoN3KXJnmAEu6tVJyvEStwIjDDIeTpWaicrxvj5JeVGUVq/QIDNSRqsxY4xr5Jwn4/R0lLmyRZwHdUBAigctCNAgUy9DvZkm+QQcDRLk2KI34/TtaQYCUoq0IIhZsfYqeg1a7Sh7NXsAAAD//xeG0VMAAAAGSURBVAMACpwqATW6NiAAAAAASUVORK5CYII=) on the top right of the table to convert it to an interactive table and then click on the columns to sort by a different statistic.\n",
    "\n",
    "\n",
    "By default, it's sorted by standard deviation, which reflects the level of disagreement among opinions.\n",
    "\n",
    "Other sorting options are:\n",
    "- `mand_rank`: The average rank of the item\n",
    "- `max_rank` / `min_rank`: The maximum/minimum rank of the item\n",
    "\n",
    "> Remember that rank is independent of the scoring and not affected by weights. A lower number is better (1 means first place)."
   ],
   "id": "c7ac6fb6d00836fc"
  },
  {
   "cell_type": "code",
   "id": "3e8f73779fc72a1f",
   "metadata": {
    "id": "3e8f73779fc72a1f"
   },
   "source": [
    "item_ranks = {item: [] for item in ITEMS}\n",
    "for vote in weighted_votes:\n",
    "    for item, rank in zip(vote['df'][EVALUATION_SUBJECT[0].upper() + EVALUATION_SUBJECT[1:]], vote['df']['Rank']):\n",
    "        item_ranks[item].append(rank)\n",
    "\n",
    "stats = {\n",
    "    item: {\n",
    "        \"mean_rank\": np.mean(ranks),\n",
    "        \"std_dev\": np.std(ranks),\n",
    "        \"min_rank\": min(ranks),\n",
    "        \"max_rank\": max(ranks)\n",
    "    }\n",
    "    for item, ranks in item_ranks.items()\n",
    "}\n",
    "\n",
    "rank_stats_df = pd.DataFrame(stats).T.sort_values(\"std_dev\", ascending=False)\n",
    "rank_stats_df\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "64237fa96d3328ba"
   },
   "cell_type": "markdown",
   "source": [
    "## Summary on scores\n",
    "\n",
    "This table displays the statistics on the accumulated score by item by criterion. You could click the ![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACQAAAAnCAIAAADo9n1hAAACkElEQVR4AeyUWXLTQBiEJcd2EhMclsJcghfuAGeBG3AGbgBngTvwwiUwBUmcxEm8ZPKp2p6RZkabo/JTXF3jf+m/W5qR1DN7/PWSPf6ezIxJbpfJ7Mb8vzJ/Z+bPRQYCUoq0INQeSP02rtaZx3RmLubmZpEs18m92cgSkFKkBQFXyJte7K/G7PLW/LvKPGKzfg1XyIz4jW1easY1skXzuy2x8T8jDDIeTsTNlqvk7NqwReFAkwqD2fjK50bMuKjzueE8fG6bnHFEkMoPRcw4Z6givf/yewdoFhGkFGv1zTheNkE9rZ8/TlpBU1qRQlAxa8GMu+Z4qXr49GHSEN4gKYLIEoCC2XyxfYPodAcr68z4BPCidGfhlJBFnNyZ3QVPKm3h+89pQ4gfrhJ3ZotVfA+//Zi2QuhEReLOzB4jvTx4FH99fScQ01KslZSiYlZiKiEk7szW9yGns4rEnRnvYGfagZDEnVlA6L7gzHppXJ2nw36xiCHZlICUIoFATCWExJ3ZgQsLZM48D3oVKS0IISTuHPoHIWdTsd8q5TYl8CpKw1XizmzYL9nHcLR9ReLO7LDfXqPxhMSdWZomx8P4tP1WqW1TAq+i1FuRRZyiMyMZDSM7yQOWB7SKlBYED1a2YMYxjg4LTL5AOyAvgSCyqhTMKD0/SgfljyWEVkAKQTvim9EYH6d6B4kfA0SQyitEzLjrF6PH+uGECFI1ZrQH/eTls933k93LxoN3KXJnmAEu6tVJyvEStwIjDDIeTpWaicrxvj5JeVGUVq/QIDNSRqsxY4xr5Jwn4/R0lLmyRZwHdUBAigctCNAgUy9DvZkm+QQcDRLk2KI34/TtaQYCUoq0IIhZsfYqeg1a7Sh7NXsAAAD//xeG0VMAAAAGSURBVAMACpwqATW6NiAAAAAASUVORK5CYII=) on the top right of the table to convert it to an interactive table and then click on the columns to sort by a different statistic.\n",
    "\n",
    "\n",
    "By default, it's sorted by standard deviation, which reflects the level of disagreement among opinions.\n",
    "\n",
    "Other sorting options are:\n",
    "- `mand_score`: The average score on that criterion of that item\n",
    "- `max_score` / `min_score`: The maximum/minimum score on that criterion of that item"
   ],
   "id": "64237fa96d3328ba"
  },
  {
   "metadata": {
    "id": "80f9b0d5af1d2c00"
   },
   "cell_type": "code",
   "source": [
    "item_scores = {f'{item}: {criteria}': [] for item in ITEMS for criteria in criteria_keys}\n",
    "for vote in weighted_votes:\n",
    "    for criteria in criteria_keys:\n",
    "        for item, score in zip(vote['df'][EVALUATION_SUBJECT[0].upper() + EVALUATION_SUBJECT[1:]], vote['df'][criteria]):\n",
    "            item_scores[f'{item}: {criteria}'].append(score)\n",
    "\n",
    "stats = {\n",
    "    item: {\n",
    "        \"mean_score\": np.mean(ranks),\n",
    "        \"std_dev\": np.std(ranks),\n",
    "        \"min_score\": min(ranks),\n",
    "        \"max_score\": max(ranks)\n",
    "    }\n",
    "    for item, ranks in item_scores.items()\n",
    "}\n",
    "\n",
    "score_stats_df = pd.DataFrame(stats).T.sort_values(\"std_dev\", ascending=False)\n",
    "score_stats_df"
   ],
   "id": "80f9b0d5af1d2c00",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# More analyzations"
   ],
   "metadata": {
    "id": "hTC1XhQ-GOLZ"
   },
   "id": "hTC1XhQ-GOLZ"
  },
  {
   "cell_type": "markdown",
   "source": "Now you can create new analytics with Gemini! Click on ![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACMAAAAgCAIAAAAXL1agAAABhklEQVR4AeyTMYqDUBRFJ+4gZSB7SCBVqpTJAtIEUqYIpEmRZaRII1hYCjYuQEsrK0H3IFi6A2cOfPiIOJ/3nSFFUD43999337vyMM73ux7n613PlPSXTU/bU9vjf1FVFaiuQhyzPWLu9zsozFA2R/1YYVEUeZ6DVl3WSU3T+L7P6kC4PMwuiYAwDMuyJACEo8Alxy4pjuPn86nnwlH01UykSbx7FEW3261tWz0RjoJOVYu/EVFSXdfX6/XxeDC6NwgFnSqeXql3NSXxpmmans/n7XabJEmvs3uligcnfrq6Jc1NSdr0L8SUNJvNdrtdEARZlu33e0MeVTw48dM16DQl6YbFYuF5Hl+a4/T9KOhU8Wj/IOl3DpoQedPj8ei6LqO5qgNHQaeqFANKk9SIw+HAl6Y4CEeBSI5dEu9+Op1WqxWjQTgKXHLskpg4n88vlwsBIBxFeKyTmLterzebDQiXnzFJy+Xy9XqB8hicY5JYHTEg/fIzJkk+veuckrrbsOWfuL0fAAAA//+KxANCAAAABklEQVQDAODm/tjPRRCRAAAAAElFTkSuQmCC) on the bottom middle of the page, and ask Gemini to create any analytics you want！",
   "metadata": {
    "id": "IxxA2dpSGWIL"
   },
   "id": "IxxA2dpSGWIL"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
